import{_ as r,C as t,c as o,o as l,j as i,G as a,a2 as c,a as h}from"./chunks/framework.CxUkksTJ.js";const v=JSON.parse('{"title":"Is It Okay to Delegate Business Logic to Infrastructure?","description":"Thoughts on delegating business logic to the infrastructure layer, through the lens of Uber embedding Rate Limiting into their service mesh","frontmatter":{"title":"Is It Okay to Delegate Business Logic to Infrastructure?","date":"2026-02-25T00:00:00.000Z","author":"Tim Kang","github":"selenehyun","description":"Thoughts on delegating business logic to the infrastructure layer, through the lens of Uber embedding Rate Limiting into their service mesh","tags":["Infrastructure","Platform Engineering"],"sidebar":false,"editLink":false,"prev":false,"next":false},"headers":[],"relativePath":"blog/business-logic-in-infrastructure.md","filePath":"blog/business-logic-in-infrastructure.md","lastUpdated":1771988382000}'),u={name:"blog/business-logic-in-infrastructure.md"};function d(p,e,b,m,g,f){const s=t("BlogPostMeta"),n=t("BlogPostFooter");return l(),o("div",null,[e[0]||(e[0]=i("h1",{id:"is-it-okay-to-delegate-business-logic-to-infrastructure",tabindex:"-1"},[h("Is It Okay to Delegate Business Logic to Infrastructure? "),i("a",{class:"header-anchor",href:"#is-it-okay-to-delegate-business-logic-to-infrastructure","aria-label":'Permalink to "Is It Okay to Delegate Business Logic to Infrastructure?"'},"​")],-1)),a(s),e[1]||(e[1]=c('<p>&quot;Is Rate Limiting business logic, or infrastructure?&quot;</p><p>Ask this to most developers and they pause for a second. You can almost see the &quot;Well, isn&#39;t it both?&quot; on their face.</p><p>Right. It&#39;s both. If request limits change based on a customer&#39;s billing plan, that&#39;s business logic. But inspecting and blocking tens of millions of requests per second in real time? That&#39;s infrastructure.</p><p>And yet Uber combined the two into a single layer. <strong>Inside the service mesh.</strong></p><h2 id="when-every-team-did-their-own-thing" tabindex="-1">When Every Team Did Their Own Thing <a class="header-anchor" href="#when-every-team-did-their-own-thing" aria-label="Permalink to &quot;When Every Team Did Their Own Thing&quot;">​</a></h2><p>In Uber&#39;s early microservices environment, Rate Limiting was each team&#39;s responsibility. Some teams built custom middleware. Others used Redis-based counters. Some just hardcoded it into their business logic.</p><p>Once the number of services hit the thousands, the predictable problems showed up.</p><p>Configurations varied across services. Operational overhead scaled linearly with the number of services. Smaller services sometimes had no rate limiting at all. <br> Managing hundreds of Redis clusters was considered, but that would have been its own operational nightmare.</p><p>This isn&#39;t unique to Uber. When you leave cross-cutting concerns to individual services in a distributed system, consistency degrades as scale grows. <br> Someone&#39;s on the latest version, someone else is on a version from a year ago, and someone doesn&#39;t even know it exists.</p><h2 id="uber-s-choice-embedding-it-in-the-service-mesh" tabindex="-1">Uber&#39;s Choice: Embedding It in the Service Mesh <a class="header-anchor" href="#uber-s-choice-embedding-it-in-the-service-mesh" aria-label="Permalink to &quot;Uber&#39;s Choice: Embedding It in the Service Mesh&quot;">​</a></h2><p><a href="https://www.uber.com/en-IN/blog/ubers-rate-limiting-system/" target="_blank" rel="noreferrer">Uber moved Rate Limiting into the service mesh layer.</a> The resulting Global Rate Limiting (GRL) system handles 80 million requests per second across more than 1,100 services.</p><p>The architecture has three tiers. A local client in each service&#39;s sidecar for immediate decisions, zone-level aggregators for metric collection, and regional/global controllers for computing the overall drop ratio.</p><p>The core design philosophy was achieving both <strong>low latency on the hot path</strong> and <strong>global consistency</strong> at the same time. Instead of per-service token buckets, the system performs probabilistic dropping based on aggregated load data. <br> It accepts a 2-3 second policy propagation delay in exchange for operational simplicity and system-wide consistency.</p><p>The results were impressive. One critical service saw up to 90% improvement in P99.5 latency, and a 15x traffic spike (22K to 367K RPS) was handled without any service outage.</p><p>But the interesting part isn&#39;t the performance numbers. <br><strong>Rate limit policies are still defined by each service team.</strong> The service mesh only handles enforcement. &quot;Which service gets what limit&quot; is a business decision. &quot;How to enforce that limit&quot; is infrastructure&#39;s job.</p><h2 id="a-similar-pattern-access-control-in-the-service-mesh" tabindex="-1">A Similar Pattern: Access Control in the Service Mesh <a class="header-anchor" href="#a-similar-pattern-access-control-in-the-service-mesh" aria-label="Permalink to &quot;A Similar Pattern: Access Control in the Service Mesh&quot;">​</a></h2><p>Uber&#39;s Rate Limiting isn&#39;t the only example. Istio&#39;s AuthorizationPolicy follows the same pattern.</p><p>&quot;Can Service A call Service B&#39;s <code>/api/orders</code>?&quot; is both an architectural decision and a business policy. Traditionally, each service handled authorization through its own middleware. <br> Istio moved this into the service mesh layer. No code changes needed. Declarative YAML. Consistent across the board. <br> Every service gets mTLS-based identity verification, and inter-service communication policies are managed centrally.</p><p>The pattern is the same. <strong>Who can access what</strong> is defined by the business/architecture team. <strong>Actually enforcing that access control</strong> is handled by infrastructure. <br> The service mesh does the plumbing. It doesn&#39;t replace business logic.</p><h2 id="another-pattern-transactional-outbox-and-debezium" tabindex="-1">Another Pattern: Transactional Outbox and Debezium <a class="header-anchor" href="#another-pattern-transactional-outbox-and-debezium" aria-label="Permalink to &quot;Another Pattern: Transactional Outbox and Debezium&quot;">​</a></h2><p>Here&#39;s one more interesting case. Event publishing.</p><p>Publishing events to Kafka in a microservices architecture is business logic. &quot;When an order completes, publish an order completion event&quot; is a business requirement. Traditionally, the application code calls the Kafka producer directly.</p><p>But this approach has a chronic problem. <br> What if the DB transaction succeeds but the Kafka publish fails? Or the other way around? Events get lost or duplicated, and data consistency across services breaks down.</p><p><a href="https://netflixtechblog.com/dblog-a-generic-change-data-capture-framework-69351fb9099b" target="_blank" rel="noreferrer">Netflix</a> built their own CDC framework called DBLog to capture real-time data changes across dozens of microservices. <br><a href="https://www.uber.com/blog/dbevents-ingestion-framework/" target="_blank" rel="noreferrer">Uber</a> built DBEvents, a CDC system that streams change logs from MySQL and Cassandra to Kafka. <br><a href="https://engineering.zalando.com/posts/2022/02/transactional-outbox-with-aws-lambda-and-dynamodb.html" target="_blank" rel="noreferrer">Zalando</a> adopted the Transactional Outbox pattern in their order system to separate synchronous data changes from asynchronous event publishing.</p><p>What they all converged on was pushing the act of event publishing itself into infrastructure. The <strong>Transactional Outbox pattern</strong>.</p><div class="language- line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>[Application] --DB write--&gt; [Outbox Table] --CDC (Debezium, etc.)--&gt; [Kafka]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The application processes business logic and writes events to the outbox table within the same transaction. <br> CDC infrastructure (Debezium, or each company&#39;s in-house solution) captures the DB change log and publishes it to Kafka.</p><p>The pattern here is the same. <strong>&quot;What events to publish&quot; is decided by the application (writing to the outbox table), while &quot;how to reliably deliver them&quot; is handled by infrastructure (CDC).</strong></p><p>The application doesn&#39;t even need to know Kafka exists. Just write to the database. <br> The benefits are dramatic. The atomicity of the DB transaction guarantees the reliability of event publishing. All the complex logic around Kafka failure handling, retries, and deduplication disappears from the application code. <br> Infrastructure guarantees at-least-once delivery.</p><h2 id="separating-definition-from-execution" tabindex="-1">Separating Definition from Execution <a class="header-anchor" href="#separating-definition-from-execution" aria-label="Permalink to &quot;Separating Definition from Execution&quot;">​</a></h2><p>A common pattern emerges across all three cases. <strong>Separating the &quot;definition&quot; and &quot;execution&quot; of business rules.</strong></p><table tabindex="0"><thead><tr><th>Domain</th><th>Business Defines</th><th>Infrastructure Executes</th></tr></thead><tbody><tr><td>Rate Limiting</td><td>Request limits per plan</td><td>Real-time request inspection and blocking</td></tr><tr><td>Access Control</td><td>Inter-service communication policies</td><td>mTLS authentication and authorization enforcement</td></tr><tr><td>Event Publishing</td><td>Which events to publish (outbox writes)</td><td>Reliable delivery (CDC to Kafka)</td></tr></tbody></table><p>&quot;Definition&quot; belongs to the business. &quot;Execution&quot; is something infrastructure can do better. Consistently, fast, reliably.</p><p>If this separation can be done cleanly, delegating execution to infrastructure is a natural choice. Having each service individually implement the &quot;execution&quot; part is actually the less efficient approach.</p><h2 id="but-historically-this-has-failed-before" tabindex="-1">But Historically, This Has Failed Before <a class="header-anchor" href="#but-historically-this-has-failed-before" aria-label="Permalink to &quot;But Historically, This Has Failed Before&quot;">​</a></h2><p>Honestly, there&#39;s an uncomfortable precedent here. <strong>ESB (Enterprise Service Bus).</strong></p><p>In the SOA era of the 2000s, ESB served as the central hub for inter-service communication. Initially it just handled message routing and protocol translation. <br> But over time, business logic crept in, one piece at a time. Data validation, business rules, conditional routing, orchestration.</p><p><a href="https://martinfowler.com/articles/microservices.html" target="_blank" rel="noreferrer">Martin Fowler and James Lewis, when introducing the microservices architecture in 2014</a>, called out the ESB problem like this:</p><blockquote><p>&quot;Smart endpoints and dumb pipes.&quot;</p></blockquote><p>The principle that services should be smart and pipes (infrastructure) should be dumb. <br> The result of putting business logic into ESB is well documented. A massive single point of failure known as the &quot;God Bus.&quot; Untestable business logic written in BPEL or XSLT. Every change funneled through the ESB team, creating an organizational bottleneck.</p><p>And this pattern keeps repeating. Start putting business logic in the API Gateway, and a few years later you&#39;ve got thousands of lines of Lua plugins that nobody understands. <br> Different generation, same mistake. You add business logic to infrastructure because it&#39;s convenient, one piece at a time, and before you know it the API Gateway has become the new ESB.</p><p>So isn&#39;t what I&#39;m talking about here just the same mistake all over again?</p><h2 id="same-mistake-or-a-different-call" tabindex="-1">Same Mistake, or a Different Call? <a class="header-anchor" href="#same-mistake-or-a-different-call" aria-label="Permalink to &quot;Same Mistake, or a Different Call?&quot;">​</a></h2><p>Well, for now at least, I think it&#39;s different. There is a key distinction.</p><p>The reason ESB failed was that the <strong>&quot;definition&quot; of business logic was also placed into infrastructure.</strong> Routing rules, data transformation rules, orchestration logic were all trapped inside ESB configuration files or proprietary DSLs. <br> When the business team wanted to change a rule, they had to file a ticket with the ESB team, and the ESB team modified configurations without understanding the business context.</p><p>In contrast, with Uber&#39;s GRL or Istio&#39;s AuthorizationPolicy, <strong>ownership of the definition stays with the business/service team.</strong></p><p>Each service team at Uber defines their own Rate Limit policies. Istio&#39;s AuthorizationPolicy is declared in YAML by the service team. Infrastructure just takes that definition and executes it.</p><p>Here&#39;s the difference:</p><table tabindex="0"><thead><tr><th></th><th>ESB (Failed)</th><th>Modern Patterns (Working)</th></tr></thead><tbody><tr><td>Definition</td><td>Infra team via ESB config</td><td>Service/business teams, declaratively</td></tr><tr><td>Execution</td><td>Infrastructure (ESB)</td><td>Infrastructure (service mesh, operators)</td></tr><tr><td>Who changes it</td><td>Infra team (bottleneck)</td><td>Service teams (autonomous)</td></tr><tr><td>How it&#39;s changed</td><td>Proprietary DSL, GUI</td><td>Standard YAML, Git-manageable</td></tr></tbody></table><p><strong>&quot;Delegating execution to infrastructure&quot; and &quot;delegating definition to infrastructure&quot; are completely different things.</strong> The lesson from ESB isn&#39;t &quot;don&#39;t delegate anything to infrastructure.&quot; It&#39;s &quot;don&#39;t hand over ownership of the definition to infrastructure.&quot;</p><h2 id="cases-where-it-still-fails" tabindex="-1">Cases Where It Still Fails <a class="header-anchor" href="#cases-where-it-still-fails" aria-label="Permalink to &quot;Cases Where It Still Fails&quot;">​</a></h2><p>Of course, separating definition from execution doesn&#39;t solve everything. There are cases where delegating execution to infrastructure still fails.</p><p><strong>When the infra team doesn&#39;t understand the business context.</strong> At one company, Istio AuthorizationPolicy was set to &quot;deny by default.&quot; A new service launch required communication with 12 services, and the platform team missed one. <br> Launch was delayed by 3 days. The platform team&#39;s response: &quot;We weren&#39;t informed about that dependency.&quot;</p><p><strong>When the boundary gets blurry.</strong> Rate limiting starts as a simple numeric cap, then expands to different policies per customer segment, different limits by time of day, per-endpoint exceptions. <br> At some point you start wondering whether it even belongs in infrastructure anymore. As Charity Majors (co-founder of Honeycomb) put it:</p><blockquote><p>&quot;The most dangerous thing in software is a layer that silently makes decisions about data it doesn&#39;t understand.&quot;</p></blockquote><p><strong>When the pool of people who can change the infrastructure shrinks.</strong> People who understand both Kubernetes and the business domain are rare. <br> Once business execution logic lives in infrastructure, the number of people who can modify it drops sharply. This is a very real risk.</p><h2 id="decision-criteria" tabindex="-1">Decision Criteria <a class="header-anchor" href="#decision-criteria" aria-label="Permalink to &quot;Decision Criteria&quot;">​</a></h2><p>So ultimately this isn&#39;t a question of &quot;should we or shouldn&#39;t we&quot; but rather &quot;is this a more manageable approach for our situation?&quot;</p><p>Here are the decision criteria I&#39;ve distilled from my experience:</p><p><strong>When it makes sense to delegate to infrastructure:</strong></p><ul><li>It&#39;s a cross-cutting concern spanning multiple services</li><li>&quot;What&quot; and &quot;How&quot; can be cleanly separated</li><li>Consistency matters more than per-service customization</li><li>Moving it to infrastructure actually reduces operational complexity</li></ul><p><strong>When it&#39;s better to keep it in each service:</strong></p><ul><li>The execution method itself is core to the business logic (e.g., three-stage payment verification)</li><li>Services need subtly different behaviors</li><li>The infra team can&#39;t realistically understand the domain</li><li>Rules change fast enough that they don&#39;t fit the infrastructure deployment cycle</li></ul><h2 id="my-experience-with-lynq" tabindex="-1">My Experience with Lynq <a class="header-anchor" href="#my-experience-with-lynq" aria-label="Permalink to &quot;My Experience with Lynq&quot;">​</a></h2><p>Building Lynq Operator, I found myself at this exact boundary.</p><p>Per-tenant Kubernetes resource provisioning is business logic. &quot;Create 2 Deployments and 1 Service for this tenant&quot; is a business requirement. But I chose to delegate this execution to a Kubernetes Operator.</p><p>The database holds business data about which tenants are active, and LynqForm templates declaratively define what resources to create for active tenants. Lynq reads both and turns them into actual Kubernetes resources.</p><p>Same pattern as Uber&#39;s GRL. Business defines, infrastructure executes.</p><p>The benefits were clear. SaaS developers just need to insert a record into the database. They don&#39;t need to know Kubernetes. <br> Safety mechanisms like Conflict Policy and Deletion Policy are applied uniformly at the infrastructure layer. Every tenant gets resources provisioned the exact same way, so there&#39;s no drift from manual work.</p><p>That said, I don&#39;t think this is the right call for every situation. If you&#39;re managing 10 tenants, a single script is probably more reasonable than building an operator.</p><h2 id="wrapping-up" tabindex="-1">Wrapping Up <a class="header-anchor" href="#wrapping-up" aria-label="Permalink to &quot;Wrapping Up&quot;">​</a></h2><p>Is it okay to delegate business logic to infrastructure?</p><p>My answer is <b><u>conditional Yes</u></b>.</p><p>If the definition and execution of business rules can be separated, if ownership of the definition stays with the business team, and if consistency and operational efficiency matter more in your situation, then delegating execution to infrastructure is a reasonable choice.</p><p>But as the history of ESB teaches, <strong>boundaries blur over time.</strong> The temptation of &quot;let&#39;s just add this one more thing...&quot; repeats, and one day you find that infrastructure has become a &quot;God Layer&quot; holding everything the business cares about.</p><p>In the end, maybe the real question isn&#39;t about infrastructure at all. It&#39;s about whether you&#39;re putting business logic into infrastructure, or just putting the runtime engine there. <br> And deciding which one it is? That&#39;s not something a framework or a best-practices doc can tell you. That&#39;s a call that people have to make. People who understand both the business and the infrastructure, who can look at the boundary and say &quot;this is where we draw the line, and here&#39;s why.&quot;</p><p>One thing is clear though: <strong>the decade-old principle that &quot;infrastructure should be dumb&quot; is too outdated to follow blindly.</strong> Service meshes, CDC, Kubernetes Operators. These aren&#39;t &quot;smart pipes.&quot; They&#39;re &quot;reliable executors&quot; that carry out business intent safely and consistently. <br> Where you draw that line is ultimately your call.</p>',77)),a(n)])}const w=r(u,[["render",d]]);export{v as __pageData,w as default};
